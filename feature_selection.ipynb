{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "+ Sometimes in a real world dataset, all features do not contribute well enough towards fitting a model. \n",
    "+ The features that do not contribute significantly, can be removed. It leads to a decrease in the size of the dataset and hence, the computation cost of fitting a model.\n",
    "+ `sklearn.feature_selction` provides many APIs to accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Based Methods\n",
    "\n",
    "##### VarianceThreshold\n",
    "\n",
    "Removes all features with variance below a certain threshold, as specified by the user,\n",
    "from the input feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'age': 4, 'height':96.0},\n",
    "{'age': 1, 'height':73.9},\n",
    "{'age': 3, 'height':88.9},\n",
    "{'age': 2, 'height':81.6}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.  96. ]\n",
      " [ 1.  73.9]\n",
      " [ 3.  88.9]\n",
      " [ 2.  81.6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.25 , 67.735])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "data_transformed = dv.fit_transform(data) #creates a 2-d matrix\n",
    "print(data_transformed)\n",
    "np.var(data_transformed, axis=0) # gives the variance of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[96. ],\n",
       "       [73.9],\n",
       "       [88.9],\n",
       "       [81.6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selects only the second column, since variance of the first column is below given threshold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(threshold=9) # set the threshold limit, by default only a feature with 0 variance will be removed\n",
    "vt.fit_transform(data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the second column has been selected because its the only one that passes the specified variance threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectKBest\n",
    "\n",
    "Select the k best features (highest scoring features) based on a given scoring method (eg: Mutual Information, chi2, f statistics)\n",
    "\n",
    "An example using the california housing dataset is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature matrix before feature selection: (2000, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, GenericUnivariateSelect, mutual_info_regression\n",
    "\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# selecting a subset of the data because the california housing dataset is quite large\n",
    "X, y = X_california[:2000, :], y_california[:2000]\n",
    "\n",
    "print(f\"The shape of the feature matrix before feature selection: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the 3 most important features using the `mutual_info_regression` scoring function\n",
    "\n",
    "Mutual Information(MI) measures the dependency between 2 variables. It returns a non-negative value:\n",
    "+ MI = 0 for independent variables\n",
    "+ Higher MI indicates higher dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the feature matrix after feature selection: (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Select 3 features using mutual_info_regression method\n",
    "skb = SelectKBest(mutual_info_regression, k=3)\n",
    "X_new = skb.fit_transform(X, y)\n",
    "\n",
    "print(f\"The shape of the feature matrix after feature selection: {X_new.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data now only has top 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x6', 'x7'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skb.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectPercentile\n",
    "\n",
    "This is very simiar to `SelectKBest`, the only difference is that it selects the the highest-scoring k% of features.\n",
    "\n",
    "It also uses a scoring function to decide the importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select top 30 percentile of the features using mutual_info_regression method\n",
    "sp = SelectPercentile(mutual_info_regression, percentile=30)\n",
    "X_new = sp.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 3 out of the 8 features got selected:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x6', 'x7'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GenericUnivariateSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 3 features using mutual_info_regression method\n",
    "gus = GenericUnivariateSelect(mutual_info_regression, mode='k_best', param=3)\n",
    "X_new = gus.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x2', 'x6', 'x7'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gus.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper based Methods\n",
    "Unlike filter based methods, wrapper based methods use estimator class rather than a scoring\n",
    "function for feature selection.\n",
    "\n",
    "#### RFE (Recursive Feature Elimination)\n",
    "\n",
    "RFE starts with all features in the training dataset and removes features recursively until\n",
    "the desired number of features are reached.\n",
    "\n",
    "+ Step 1: Fits a model and\n",
    "+ Step 2: Ranks the features, afterwards it removes one ore more features (depending upon the `step` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False False False False False  True  True]\n",
      "Rank of each feature are [1 5 4 3 6 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "# RFE takes the estimator and the number of features to select as input\n",
    "selector = RFE(estimator, n_features_to_select=3, step=1) # select 3 features\n",
    "selector = selector.fit(X, y)\n",
    "\n",
    "# .support_ attribute returns a boolean array where True indicates selected features\n",
    "print(selector.support_)  # True indicates selected columns\n",
    "\n",
    "# .ranking_ attribute gives the rank of each feature\n",
    "# if its value is '1', then it is selected,\n",
    "# features with rank 2 and above are not selected\n",
    "print(f\"Rank of each feature are {selector.ranking_}\") # rank 1 assigned to only selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above, we can see that the 1st, 7th and the 8th features are selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = selector.transform(X) # returns only the previously \"selected\" columns\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFECV\n",
    "\n",
    "RFECV adds another layer of cross validationto RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectFromModel\n",
    "\n",
    "Selects the desired number of important features (as specified with `max_features` parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of features: [ 3.64048292e-01  5.56221906e-03  5.13591243e-02 -1.64474348e-01\n",
      "  5.90411479e-05 -1.64573915e-01 -2.17724525e-01 -1.85343265e-01]\n",
      "Indices of top 3 features: [1 2 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "estimator.fit(X, y)\n",
    "\n",
    "print(f'Coefficients of features: {estimator.coef_}')\n",
    "print(f'Indices of top 3 features: {np.argsort(estimator.coef_)[-3:]}')\n",
    "\n",
    "model = SelectFromModel(estimator, max_features=3, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = selector.transform(X) # returns only the previously \"selected\" columns\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SequentialFeatureSelector\n",
    "\n",
    "It performs feature selection by selecting or deselecting features one by one in a greedy manner.\n",
    "It uses one of two approaches:\n",
    "+ Forward Selection\n",
    "+ Backward Selection\n",
    "\n",
    "The `direction` parameter controls whether Forward or backward selction is used and in general, the two do not yield the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 296 ms, sys: 1.07 ms, total: 297 ms\n",
      "Wall time: 296 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Forward sequence selector\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3)\n",
    "X_new = sfs.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 selected features can be observed by using the `get_feature_names_out()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x5', 'x6'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 ms, sys: 165 ms, total: 735 ms\n",
      "Wall time: 506 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Backward sequence selector - depends on n_features_to_select.\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "estimator = LinearRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(estimator, n_features_to_select=3, direction='backward')\n",
    "X_new = sfs.fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['x0', 'x5', 'x6'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sfs.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in this case forward selection is faster because it will only need to go through 3 iterations whereas backward selection will need to go through 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
